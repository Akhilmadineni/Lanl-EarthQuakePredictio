---
# MLProject-LANL earthQake prediction Randomforest
Akhil Madineni
April 19, 2019
# We will use random forest since they are easy to tune and work well with a lot of highly correlated features.
# So let's start by loading a couple of packages, defining some functions as well as constants that we will use later.
```{r}
library(data.table)
library(roll)
library(ranger)
library(tidyverse)
```

# ===============================================================
# FUNCTIONS
# ===============================================================
# Mean absolute error



```{r}
mae <- function(y, pred) {
  mean(abs(y - pred))
}

```

# Calculates the coefficient of an AR(1) process z
```{r}
ar1 <- function(z) {
  cor(z[-length(z)], z[-1])  
}
```

# Calculates a buntch of statistics from vector x
```{r}
univariate_stats <- function(x, tag = NULL, p = c(0, 0.25, 0.75, 1)) {
  x <- x[!is.na(x)]
 
  out <- c(
    mean = mean(x),
    sd = sd(x),
    setNames(quantile(x, p = p, names = FALSE), paste0("q", p)),
    ar1 = ar1(x))
  
  if (is.null(tag)) 
    return(out)
  
  names(out) <- paste(names(out), tag, sep = "_")
  out
}
```

# Feature extraction on vector x. Basically calls "univariate_stats" on differently transformed x
```{r}
create_X <- function(x, rolling_windows = c(10, 1000)) {
  stats_full <- univariate_stats(x = x, "full", p = c(0, 1, 5, 10, 25, 50, 75, 90, 95, 99, 100) / 100)
  stats_abs <- univariate_stats(x = abs(x), "abs")

  # Rolling versions of x
  x_mat <- as.matrix(x, ncol = 1)
  roll_sd_k <- lapply(rolling_windows, function(k) roll_sd(x_mat, width = k))
  
  # Derive stats from rolling versions
  stats_roll_sd <- Map(univariate_stats, roll_sd_k, tag = paste("roll_sd", rolling_windows, sep = "_"))
  
  c(stats_full, stats_abs, unlist(stats_roll_sd))
}
```

# ===============================================================
# CONSTANTS
# ===============================================================


```{r}
# Length of test data sets
n_test <- 150000

# By how much do we shift the time window of n_test rows within earthquake?
stride <- 150000

# Positions of earthquakes (see answers in https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/77390). Used to create contiguous folds for cross-validation and train/validation split
earthquakes <- c(
    5656573,
   50085877,
  104677355,
  138772452,
  187641819,
  218652629,
  245829584,
  307838916,
  338276286,
  375377847,
  419368879,
  461811622,
  495800224,
  528777114,
  585568143,
  621985672) + 1
```

# Figure out colnames of input as well as number of features
```{r}
raw <- fread(file.path(".", "Data", "train.csv"), nrows = 150000, data.table = FALSE)
names_input <- names(raw)
names_features <- names(create_X(raw[[1]]))

names_features
```

# If not yet created, make directory to save folds
```{r}
fold_dir <- file.path("strides", stride)
if (!dir.exists(fold_dir)) {
  dir.create(fold_dir, recursive = TRUE)  
}
```

# So let's create one data set per earthquake as follows:
# Load all rows before an earthquake occurs.
# For chunks of size 150'000 (like test data), we extract a couple of features and store them in a row of a matrix X. The response is stored in a vector y.
# We move by "stride" positions and repeat steps 2 & 3 until the earthquake happens.
```{r}
for (prep_fold in seq_along(earthquakes)) {# prep_fold <- 1
 cat(prep_fold, "\n")
  
  # Read data between two earthquakes
  raw <- fread(file.path(".", "Data", "train.csv"), 
               nrows = c(earthquakes[1], diff(earthquakes))[prep_fold], 
               skip = c(0, earthquakes)[prep_fold] + 1)
  setnames(raw, names_input)
  
  # How many times do we calculate features for this data chunk?
  n_steps <- (nrow(raw) - n_test) %/% stride
  
  # Init feature matrix and vector of response
  y <- numeric(n_steps)
  X <- matrix(NA, nrow = n_steps, ncol = length(names_features), dimnames = list(NULL, names_features))
  
  # Loop through chunk and build up y and X
  pb <- txtProgressBar(0, n_steps, style = 3)
  
  for (i in seq_len(n_steps)) {
    setTxtProgressBar(pb, i)
    from <- 1 + stride * (i - 1)
    to <- n_test + stride * (i - 1)
    X[i, ] <- create_X(raw$acoustic_data[from:to])
    y[i] <- raw$time_to_failure[to]
  }

  save(y, X, file = file.path(fold_dir, paste0("fold_", prep_fold, ".RData")))
}
```


# Now, the data preparation is over and we can load all saved .RData files to create the full data set. Each .RData (= each earthquake) will serve as one fold in our leave-one-earthquake-out cross-validation strategy.
```{r}
for (i in seq_along(earthquakes)) {
  load(file.path("strides", stride, paste0("fold_", i, ".RData")))
  
  fold <- rep(i, length(y))
  
  if (i == 1) {
    X_mat <- X
    y_vec <- y
    fold_vec <- fold 
  } else {
    X_mat <- rbind(X_mat, X)
    y_vec <- c(y_vec, y)
    fold_vec <- c(fold_vec, fold)
  }
}
```

```{r}
form <- reformulate(colnames(X_mat), "label")
fullDF <- data.frame(label = y_vec, X_mat)
fullDF
```

# Now, we will use above mentioned cross-validation strategy to evaluate the quality of our random forest model.

```{r}
# cross-validation
m_fold <- length(earthquakes)
cv <- numeric(m_fold)
pb <- txtProgressBar(0, m_fold, style = 3)
```

```{r}
for (j in seq_along(cv)) { # j <- 1
  j
  setTxtProgressBar(pb, j)
  fit <- ranger(form, fullDF[fold_vec != j, ],num.trees = 500, seed = 3564 + 54 * j, verbose = 2)
  cv[j] <- mae(fullDF[fold_vec == j, "label"], predict(fit, fullDF[fold_vec == j, ])$predictions)
}
```

# Resulting score
```{r}
mean(cv)
```

```{r}
weighted.mean(cv, w = tabulate(fold_vec, nbins = m_fold))
```

```{r}
m_fold <- length(earthquakes)
cv <- numeric(m_fold)
pb <- txtProgressBar(0, m_fold, style = 3)
```

```{r}
for (j in seq_along(cv)) { # j <- 1
  j
  setTxtProgressBar(pb, j)
  fit <- ranger(form, fullDF[fold_vec != j, ],num.trees = 2000, seed = 3564 + 54 * j, verbose = 2)
  cv[j] <- mae(fullDF[fold_vec == j, "label"], predict(fit, fullDF[fold_vec == j, ])$predictions)
}
```

# Resulting score
```{r}
mean(cv)
```

```{r}
weighted.mean(cv, w = tabulate(fold_vec, nbins = m_fold))
```

```{r}
# cross-validation
m_fold <- length(earthquakes)
cv <- numeric(m_fold)
pb <- txtProgressBar(0, m_fold, style = 3)
```

```{r}
for (j in seq_along(cv)) { # j <- 1
  j
  setTxtProgressBar(pb, j)
  fit <- ranger(form, fullDF[fold_vec != j, ],num.trees = 5000, seed = 3564 + 54 * j, verbose = 2)
  cv[j] <- mae(fullDF[fold_vec == j, "label"], predict(fit, fullDF[fold_vec == j, ])$predictions)
}
```
# Resulting score
```{r}
mean(cv)
```

```{r}
weighted.mean(cv, w = tabulate(fold_vec, nbins = m_fold))
```


```{r}
retrain on full data for submission
```


```{r}
fit_rf <- ranger(form, fullDF, importance = "impurity", seed = 345)
fit_rf
```

# Variable importance
```{r}
par(mar = c(5, 10, 1, 1))
barplot(importance(fit_rf) %>% sort %>% tail(70), horiz = T, las = 1,col = "darkred",beside = TRUE)
```

```{r}
# submission
submission <- fread(file.path(".", "Data", "sample_submission.csv"))

# Load each test data and create the feature matrix. Takes 2-3 minutes and can be skipped
# if playing with strides (and not with features)
load_and_prepare <- function(file) {
  seg <- fread(file.path(".", "Data", "test", paste0(file, ".csv")))
  create_X(seg$acoustic_data)
}
all_test <- lapply(submission$seg_id, load_and_prepare)
all_test2 <- do.call(rbind, all_test)
save(all_test2, file = "test_prep.RData")
# load("test_prep.RData") 
dim(all_test2) # 2624   35
```

```{r}
submission$time_to_failure <- predict(fit_rf, data.frame(all_test2))$prediction

head(submission)
```

```{r}
# Save
fwrite(submission, paste0("submission_rf_stride_", stride, ".csv"))
```































































